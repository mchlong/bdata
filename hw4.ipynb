{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: linearmodels in /Users/wolong/anaconda3/lib/python3.11/site-packages (5.4)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (1.12.0)\n",
      "Requirement already satisfied: statsmodels>=0.12.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (0.13.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (0.4.3)\n",
      "Requirement already satisfied: Cython>=0.29.37 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (3.0.10)\n",
      "Requirement already satisfied: pyhdfe>=0.1 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (0.2.0)\n",
      "Requirement already satisfied: formulaic>=0.6.5 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (1.0.1)\n",
      "Requirement already satisfied: setuptools-scm[toml]<9.0.0,>=8.0.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from linearmodels) (8.0.4)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from formulaic>=0.6.5->linearmodels) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from formulaic>=0.6.5->linearmodels) (4.6.3)\n",
      "Requirement already satisfied: wrapt>=1.0 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from formulaic>=0.6.5->linearmodels) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.0->linearmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.0->linearmodels) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.0->linearmodels) (2023.3)\n",
      "Requirement already satisfied: packaging>=20 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (23.0)\n",
      "Requirement already satisfied: setuptools in /Users/wolong/anaconda3/lib/python3.11/site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (67.8.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/wolong/anaconda3/lib/python3.11/site-packages (from statsmodels>=0.12.0->linearmodels) (0.5.3)\n",
      "Requirement already satisfied: six in /Users/wolong/anaconda3/lib/python3.11/site-packages (from patsy>=0.5.2->statsmodels>=0.12.0->linearmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install linearmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from scipy.stats.mstats import winsorize\n",
    "from linearmodels.panel import PanelOLS\n",
    "import wrds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowSensitivity(object):\n",
    "    \"\"\"\n",
    "    Class to replicate the flow sensitivity analysis in Goldstein et al. (2017).\n",
    "    You will need to implement the methods in this class, wherever there is a TODO.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.db = None\n",
    "        self.merge = None\n",
    "\n",
    "    def _download_fund_summary(self, start_date='1992-01-01', end_date='2014-12-31'):\n",
    "        \"\"\"\n",
    "        Download the fund summary data from WRDS\n",
    "        \"\"\"\n",
    "        print('Downloading fund_summary_full...')\n",
    "        t0 = time.time()\n",
    "        df_fund_summary_query = f\"SELECT * FROM crsp.fund_summary2 WHERE (caldt BETWEEN '{start_date}' AND '{end_date}')\"\n",
    "        df = self.db.raw_sql(df_fund_summary_query)\n",
    "        df.to_csv(\"fund_summary_full.csv\", index=False)\n",
    "        t1 = time.time()\n",
    "        print(\"Downloaded fund_summary_full Successfully in\", t1 - t0, 's')\n",
    "        return None\n",
    "\n",
    "    def _download_monthly_returns(self, start_date='1992-01-01', end_date='2014-12-31'):\n",
    "        \"\"\"\n",
    "        Download the monthly returns data from WRDS\n",
    "        \"\"\"\n",
    "        print('Downloading MonthlyReturns')\n",
    "        t0 = time.time()\n",
    "        df_query = f\"SELECT * FROM crsp.monthly_tna_ret_nav WHERE (caldt BETWEEN '{start_date}' AND '{end_date}')\"\n",
    "        df = self.db.raw_sql(df_query)\n",
    "        df.to_csv(\"MonthlyReturns.csv\", index=False)\n",
    "        t1 = time.time()\n",
    "        print(\"Downloaded MonthlyReturns Successfully in\", t1 - t0, 's')\n",
    "\n",
    "    def crsp_data_download(self, username, start_date='1992-01-01', end_date='2014-12-31'):\n",
    "        \"\"\"\n",
    "        Connect to WRDS and download the data\n",
    "        Note: you do NOT need to change any code here\n",
    "\n",
    "        \"\"\"\n",
    "        self.db = wrds.Connection(wrds_username=username)\n",
    "\n",
    "        self._download_fund_summary(start_date=start_date, end_date=end_date)\n",
    "        self._download_monthly_returns(start_date=start_date, end_date=end_date)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _import_clean_mret(self):\n",
    "        \"\"\"\n",
    "        Load and clean MonthlyReturns.csv\n",
    "\n",
    "        \"\"\"\n",
    "        mret = pd.read_csv(\"MonthlyReturns.csv\")\n",
    "        # Rename specified columns\n",
    "        mret.rename(columns={'crsp_fundno': 'Id', 'mtna': 'tna', 'mret': 'ret'}, inplace=True)\n",
    "        # Convert 'caldt' column to datetime format\n",
    "        mret['caldt'] = pd.to_datetime(mret['caldt'])\n",
    "        # Create a new column 'date' (first day in the month of 'caldt') to merge with other datasets\n",
    "        mret['date'] = mret['caldt'].dt.to_period('M').dt.to_timestamp()\n",
    "        # Check primary key\n",
    "        assert mret.duplicated(subset=['date', 'Id']).sum() == 0\n",
    "\n",
    "        return mret\n",
    "\n",
    "    def balanced_panel(self, mret):\n",
    "        \"\"\"\n",
    "        Fills in skipping monthly dates for each unique Id in the DataFrame.\n",
    "\n",
    "        \"\"\"\n",
    "        # Ensure 'date' is in datetime format and sort\n",
    "        mret.sort_values(by=['Id', 'date'], inplace=True)\n",
    "        # Generate a complete sequence of monthly dates for each Id\n",
    "        def generate_date_range(group):\n",
    "            min_date = group['date'].min()\n",
    "            max_date = group['date'].max()\n",
    "            all_dates = pd.date_range(start=min_date, end=max_date, freq='MS')  # MS is month start frequency\n",
    "            return pd.DataFrame({'Id': group['Id'].iloc[0], 'date': all_dates})\n",
    "        # Apply function to each group and concatenate the results\n",
    "        mret_new = pd.concat([generate_date_range(group) for _, group in mret.groupby('Id')])\n",
    "        # Merge back to include the newly generated dates\n",
    "        mret_new = pd.merge(mret_new, mret, on=['Id', 'date'], how='left')\n",
    "        # Sort and reset index\n",
    "        mret_new.sort_values(by=['Id', 'date'], inplace=True)\n",
    "        mret_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return mret_new\n",
    "\n",
    "\n",
    "    def _import_clean_qinfo(self):\n",
    "        # Import data\n",
    "        qinfo = pd.read_csv(\"fund_summary_full.csv\")\n",
    "        # Select and rename specified columns\n",
    "        qinfo = qinfo[['crsp_fundno', 'caldt', 'exp_ratio', 'first_offer_dt',\n",
    "                        'lipper_obj_cd', 'si_obj_cd', 'wbrger_obj_cd', 'crsp_obj_cd',\n",
    "                        'index_fund_flag', 'et_flag', 'crsp_cl_grp']]\n",
    "        qinfo.rename(columns={\n",
    "            'crsp_fundno': 'Id',\n",
    "            'caldt': 'date',\n",
    "            'exp_ratio': 'expenseratio'\n",
    "        }, inplace=True)\n",
    "        # Convert dates and handle missing values\n",
    "        qinfo['date'] = pd.to_datetime(qinfo['date']).dt.to_period('M').dt.to_timestamp()\n",
    "        qinfo['first_offer_dt'] = pd.to_datetime(qinfo['first_offer_dt'], errors='coerce')\n",
    "        # Handle NA values for numeric and string columns differently\n",
    "        qinfo['expenseratio'] = qinfo['expenseratio'].fillna(-999)\n",
    "        string_columns = ['index_fund_flag', 'et_flag', 'lipper_obj_cd', 'si_obj_cd', 'wbrger_obj_cd', 'crsp_obj_cd']\n",
    "        qinfo[string_columns] = qinfo[string_columns].fillna(\"\")\n",
    "        # Drop duplicates\n",
    "        qinfo = qinfo.groupby(['Id', 'date']).first().reset_index()\n",
    "        # Check primary key\n",
    "        assert qinfo.duplicated(subset=['date', 'Id']).sum() == 0\n",
    "\n",
    "        return qinfo\n",
    "    \n",
    "    def _import_clean_benchmarkrate(self):\n",
    "        # Import data\n",
    "        benchmarkrate = pd.read_csv(\"stage1_vars.csv\")\n",
    "        # Convert dates\n",
    "        benchmarkrate['date'] = pd.to_datetime(benchmarkrate['date'])\n",
    "\n",
    "        return benchmarkrate\n",
    "\n",
    "    def import_merge_and_standarize_data(self):\n",
    "        \"\"\"\n",
    "        Merges mret with qinfo and benchmarkrate, then fills forward missing values\n",
    "        in specified columns.\n",
    "\n",
    "        \"\"\"\n",
    "        # Import data\n",
    "        mret = self._import_clean_mret()\n",
    "        mret = self.balanced_panel(mret)\n",
    "        qinfo = self._import_clean_qinfo()\n",
    "        benchmarkrate = self._import_clean_benchmarkrate()\n",
    "        # Perform left joins based on 'date', and then on both 'date' and 'Id'\n",
    "        merge = pd.merge(mret, qinfo, on=['date', 'Id'], how='left')\n",
    "        merge = pd.merge(merge, benchmarkrate, on='date', how='left')\n",
    "        # Sort by 'Id' and 'date'\n",
    "        merge.sort_values(by=['Id', 'date'], inplace=True)\n",
    "        # Fill forward missing values in specified columns\n",
    "        columns_to_fill = ['expenseratio', 'first_offer_dt', 'lipper_obj_cd', 'si_obj_cd', \n",
    "                           'wbrger_obj_cd', 'crsp_obj_cd', 'index_fund_flag', 'et_flag']\n",
    "        merge[columns_to_fill] = merge.groupby('Id')[columns_to_fill].fillna(method='bfill')\n",
    "        \n",
    "        return merge\n",
    "\n",
    "    def calculate_flow_ratio(self, merge):\n",
    "        \"\"\"\n",
    "        Calculates the flow ratio and one-period lagged flow ratio for each row in the DataFrame\n",
    "        based on total net assets (tna), return (ret), and an identifier (Id).\n",
    "        The flow ratio is calculated as:\n",
    "            (tna - lag(tna) * (1 + ret)) / lag(tna)\n",
    "        \"\"\"\n",
    "        # TODO: Write code here to calculates the flow ratio and one period lagged flow ratio        \n",
    "        merge['prev_tna'] = merge.groupby('Id')['tna'].shift()\n",
    "        merge['flow_ratio'] = (merge['tna'] - merge['prev_tna'] * (1 + merge['ret'])) / merge['prev_tna']\n",
    "        merge['lagged_flow_ratio'] = merge.groupby('Id')['flow_ratio'].shift()\n",
    "\n",
    "        return merge\n",
    "\n",
    "    def calculate_excess_return(self, merge):\n",
    "        # TODO: Write code here to calculates excess returns\n",
    "        merge['ret_rf'] = merge['ret'].subtract(merge['rf'])\n",
    "\n",
    "        return merge\n",
    "    \n",
    "    def filter_corporate_nonPassiveETF(self, merge):\n",
    "        \"\"\"\n",
    "        Filters and adjusts the merge DataFrame based on various criteria.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Write code here to filter for corporate, non-passive and non-ETF/ETN funds\n",
    "        exclude_conditions = {\n",
    "            'et_flag': {'F', 'N'},\n",
    "            'index_fund_flag': {'B', 'D', 'E'}\n",
    "        }\n",
    "        # Applying filter conditions\n",
    "        for column, values in exclude_conditions.items():\n",
    "            merge = merge[~merge[column].isin(values)]\n",
    "        \n",
    "        corporate_bond_criteria = (\n",
    "            merge['lipper_obj_cd'].isin({'A', 'BBB', 'HY', 'SII', 'SID', 'IID'}) |\n",
    "            merge['si_obj_cd'].isin({'CGN', 'CHQ', 'CHY', 'CIM', 'CMQ', 'CPR', 'CSM'}) |\n",
    "            merge['wbrger_obj_cd'].isin({'CBD', 'CHY'}) |\n",
    "            merge['crsp_obj_cd'].str.startswith('IC')\n",
    "        )\n",
    "        merge = merge[corporate_bond_criteria]\n",
    "\n",
    "        return merge\n",
    "\n",
    "    def filter_ids_with_insufficient_data(self, merge, min_periods=12):\n",
    "        \"\"\"\n",
    "        Filter DataFrame to include only Ids with at least `min_periods`\n",
    "        non-missing entries in the 'ret_rf' column.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Write code here to include only Ids with at least `min_periods`\n",
    "        #       non-missing entries in the 'ret_rf' column.\n",
    "        valid_ids = merge.groupby('Id').filter(lambda x: x['ret_rf'].count() >= min_periods)['Id'].unique()\n",
    "        merge = merge[merge['Id'].isin(valid_ids)]\n",
    "        return merge\n",
    "    \n",
    "    def rolling_capm(self, group,group_id):\n",
    "        \"\"\"\n",
    "        Executes a rolling CAPM regression for each unique Id within the group.\n",
    "        This function calculates alpha and beta coefficients over a specified rolling window.\n",
    "        Parameters:\n",
    "        - group: DataFrame for a specific mutual fund (grouped by Id).\n",
    "        \"\"\"\n",
    "        # Setting the rolling window size for the regression\n",
    "        rolling_window = 12\n",
    "\n",
    "        # Prepare independent variables with a constant added for the intercept (alpha)\n",
    "        independent_vars = sm.add_constant(group[['mkt_rf', 'vbnd_ret_rf']])\n",
    "        dependent_var = group['ret_rf']\n",
    "\n",
    "        # Prepare the DataFrame to store rolling regression outcomes\n",
    "        group['alpha_next'] = np.nan\n",
    "        group['beta_bond_next'] = np.nan\n",
    "        group['beta_market_next'] = np.nan\n",
    "\n",
    "        # Instantiate the rolling OLS model to calculate rolling regression parameters\n",
    "        rolling_model = RollingOLS(dependent_var, independent_vars, window=rolling_window)\n",
    "        results = rolling_model.fit()\n",
    "\n",
    "        # Extracting regression coefficients and storing them\n",
    "        group.loc[results.params.index, 'alpha_next'] = results.params['const'].values\n",
    "        group.loc[results.params.index, 'beta_bond_next'] = results.params['vbnd_ret_rf'].values\n",
    "        group.loc[results.params.index, 'beta_market_next'] = results.params['mkt_rf'].values\n",
    "\n",
    "        # Shifting the calculated parameters to use them as predictors for the next period\n",
    "        group['alpha_next'] = group['alpha_next'].shift()\n",
    "        group['beta_bond_next'] = group['beta_bond_next'].shift()\n",
    "        group['beta_market_next'] = group['beta_market_next'].shift()\n",
    "\n",
    "        return group\n",
    "    \n",
    "    \n",
    "    def lag_capm_paras(self, merge):\n",
    "        # Ensure 'date' is in datetime format and sort values\n",
    "        merge = merge.sort_values(by=['Id', 'date'])\n",
    "        # Group by 'Id' and calculate lagged values\n",
    "        merge['alpha'] = merge.groupby('Id')['alpha_next'].shift(1)\n",
    "        merge['beta_bnd'] = merge.groupby('Id')['beta_bond_next'].shift(1)\n",
    "        merge['beta_equity'] = merge.groupby('Id')['beta_market_next'].shift(1)\n",
    "\n",
    "        return merge\n",
    "    \n",
    "\n",
    "    def rolling_regression(self, merge):\n",
    "        # Balnaced panel\n",
    "        merge = self.balanced_panel(merge)\n",
    "        # Apply the filtering to ensure sufficient non-missing 'ret_rf' data\n",
    "        merge = self.filter_ids_with_insufficient_data(merge)\n",
    "        # Apply the rolling regression function\n",
    "        merge = merge.groupby('Id').apply(lambda x: self.rolling_capm(x, x.name)).reset_index(drop=True)\n",
    "        # lag one period so that we don't use unavailable information\n",
    "        merge = self.lag_capm_paras(merge)\n",
    "\n",
    "        return merge\n",
    "    \n",
    "\n",
    "    def filter_out_early_last_history(self, merge):\n",
    "        \"\"\"\n",
    "        Filters and adjusts the merge DataFrame based on various criteria.\n",
    "        \"\"\"\n",
    "        # Filter out first year and last year history of each mutual fund\n",
    "        ## Calculate age in years\n",
    "        merge['age'] = (pd.to_datetime(merge['date']) - pd.to_datetime(merge['first_offer_dt'])).dt.days / 365\n",
    "        ## Calculate the 'end_age' as the number of years between 'date' and 'last_date'\n",
    "        merge['last_date'] = merge.groupby('Id')['date'].transform('max')\n",
    "        merge['end_age'] = (merge['last_date'] - merge['date']).dt.days / 365\n",
    "        # Apply filters based on the provided lists and conditions\n",
    "        conditions = (\n",
    "            (merge['age'] > 1) &\n",
    "            (merge['end_age'] > 1) \n",
    "        )\n",
    "        merge = merge.loc[conditions]\n",
    "\n",
    "        return merge\n",
    "\n",
    "    def select_merged_data(self, merge):\n",
    "        \"\"\"\n",
    "        Keep specific columns and drop NAs and Infs\n",
    "\n",
    "        \"\"\"\n",
    "        # select specific columns\n",
    "        merge = merge[['Id', 'date', 'flow_ratio', 'lagged_flow_ratio',\n",
    "                    'alpha','beta_bnd', 'beta_equity',\n",
    "                    'age', 'tna', 'expenseratio']]\n",
    "        # drop NAs (negative expenseratio is also NAs) and Infs\n",
    "        merge = merge[merge['expenseratio'] > 0]\n",
    "        merge.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        merge.dropna(inplace=True)\n",
    "\n",
    "        return merge\n",
    "\n",
    "    def winsorize_merged_data(self, merge):\n",
    "        # TODO: Write code here to winsorize flow ratio and alpha at 1% and 99% level\n",
    "        for column in ['flow_ratio', 'alpha','lagged_flow_ratio']:\n",
    "            merge[f'{column}_w'] = winsorize(merge[column], limits=[0.01, 0.01])\n",
    "\n",
    "        return merge\n",
    "\n",
    "    def generate_all_variables(self, merge):\n",
    "        merge['log_tna'] = np.log(merge['tna'])\n",
    "        merge['log_age'] = np.log(merge['age'])\n",
    "        merge['neg_alpha'] = (merge['alpha'] < 0).astype(int)\n",
    "        merge['alpha_w_neg_alpha'] = merge['alpha_w'] * merge['neg_alpha']\n",
    "        # drop NAs and Infs\n",
    "        merge.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        merge.dropna(inplace=True)\n",
    "\n",
    "        return merge\n",
    "\n",
    "    def flow_sensitivity_regression(self, merge):\n",
    "        # TODO: write code here to run the regression and print the results\n",
    "\n",
    "        merge.set_index(['Id', 'date'], inplace=True)\n",
    "\n",
    "        regression_formula = 'flow_ratio_w ~ 1 + alpha_w + alpha_w_neg_alpha + neg_alpha + lagged_flow_ratio_w + log_tna + log_age + expenseratio + TimeEffects'\n",
    "        regression_model = PanelOLS.from_formula(regression_formula, data=merge)\n",
    "        regression_results = regression_model.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "        print(regression_results)\n",
    "        return regression_results\n",
    "\n",
    "    def save_results(self, results):\n",
    "        \"\"\"\n",
    "        Save the results to a text file.\n",
    "        Note: the format *does not matter*, as long as the results are printed in an\n",
    "             understandable and readable format. Just printing the regression results\n",
    "             /output directly and saving that to the file is enough\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Upload this file along with the submission template to the autograder\n",
    "        #       (please do not change the name of the file)\n",
    "\n",
    "        f = open(\"flow_sensitivity_regression_results.txt\", \"w\")\n",
    "        print(results, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n",
      "Downloading fund_summary_full...\n"
     ]
    }
   ],
   "source": [
    "hw = FlowSensitivity()\n",
    "\n",
    "# To download the files \"MonthlyReturns.csv\" and \"fund_summary_full.csv\" from WRDS\n",
    "# If you have already saved these files, you can comment out the following lines\n",
    "# NOTE: they must be in the same directory as this script, with the same names\n",
    "# TODO: make sure to change the username, and pick relevant start and end dates\n",
    "wrds_username = 'wl2834'\n",
    "hw.crsp_data_download(wrds_username, start_date='1992-01-01', end_date='2014-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:           flow_ratio_w   R-squared:                        0.0567\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.3277\n",
      "No. Observations:               16021   R-squared (Within):              -0.0362\n",
      "Date:                Fri, Apr 05 2024   R-squared (Overall):              0.0557\n",
      "Time:                        15:10:37   Log-likelihood                 1.987e+04\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      137.55\n",
      "Entities:                        1694   P-value                           0.0000\n",
      "Avg Obs:                       9.4575   Distribution:                 F(7,16004)\n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                      10.0000   F-statistic (robust):             54.087\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                      10   Distribution:                 F(7,16004)\n",
      "Avg Obs:                       1602.1                                           \n",
      "Min Obs:                       1559.0                                           \n",
      "Max Obs:                       1632.0                                           \n",
      "                                                                                \n",
      "                                  Parameter Estimates                                  \n",
      "=======================================================================================\n",
      "                     Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               0.0261     0.0034     7.6157     0.0000      0.0194      0.0328\n",
      "alpha_w                 1.7830     0.3597     4.9567     0.0000      1.0779      2.4881\n",
      "alpha_w_neg_alpha       1.0611     5.4414     0.1950     0.8454     -9.6046      11.727\n",
      "neg_alpha              -0.0008     0.0034    -0.2480     0.8041     -0.0075      0.0058\n",
      "lagged_flow_ratio_w     0.1075     0.0211     5.1064     0.0000      0.0662      0.1488\n",
      "log_tna                 0.0014     0.0004     3.9504     0.0001      0.0007      0.0021\n",
      "log_age                -0.0146     0.0012    -11.694     0.0000     -0.0170     -0.0121\n",
      "expenseratio           -0.9149     0.1863    -4.9112     0.0000     -1.2800     -0.5497\n",
      "=======================================================================================\n",
      "\n",
      "F-test for Poolability: 17.468\n",
      "P-value: 0.0000\n",
      "Distribution: F(9,16004)\n",
      "\n",
      "Included effects: Time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hw = FlowSensitivity()\n",
    "# Homework begins here:\n",
    "# Step 1: Download and clean data\n",
    "merge = hw.import_merge_and_standarize_data()\n",
    "\n",
    "# Step 2: Filter to just have corporate bond mutual funds\n",
    "merge = hw.filter_corporate_nonPassiveETF(merge)\n",
    "\n",
    "# Step 3: Calculate flow ratio and excess return\n",
    "merge = hw.calculate_flow_ratio(merge)\n",
    "merge = hw.calculate_excess_return(merge)\n",
    "\n",
    "# Step 4: Run the rolling CAPM regression to calculate alpha and beta\n",
    "merge = hw.rolling_regression(merge)\n",
    "\n",
    "# Step 5 + some further data cleanup (filter, winsorize, lag variables)\n",
    "merge = hw.filter_out_early_last_history(merge)\n",
    "merge = hw.select_merged_data(merge)\n",
    "merge = hw.winsorize_merged_data(merge)\n",
    "merge = hw.generate_all_variables(merge)\n",
    "\n",
    "# Step 6: Estimation\n",
    "results = hw.flow_sensitivity_regression(merge)\n",
    "hw.save_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
