{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the submission template for the students\n",
    "'''\n",
    "# Place your imports here\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_q1q2 = 'News.RTRS.200809.0214.txt'\n",
    "file_q3 = 'News.RTRS.202006.0214.txt'\n",
    "lm_dict_file = 'Loughran-McDonald_MasterDictionary_1993-2021.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatorException(Exception):\n",
    "    \"\"\"A class to throw if you come across incorrect syntax or other issues\"\"\"\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q1a_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results: \n",
    "    How many articles are there in the JSON file for September 2008?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    return 287688\n",
    "\n",
    "def test_q1a_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    How many articles are there in the JSON file named \"filename\"?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    fp = open(filename, 'r')\n",
    "    data=json.load(fp)\n",
    "    return len(data['Items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 (a):\n",
      "specific case: 287688\n",
      "generic case: 287688\n"
     ]
    }
   ],
   "source": [
    "print('Q1 (a):')\n",
    "num1 = test_q1a_specific(0)\n",
    "num2 = test_q1a_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 (b):\n",
      "specific case: 189911\n",
      "generic case: 189911\n"
     ]
    }
   ],
   "source": [
    "def test_q1b_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results: \n",
    "    How many of articles in the JSON file for September 2008 are in English?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    return 189911\n",
    "\n",
    "def test_q1b_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    How many of articles in the JSON file named \"filename\" are in English?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    fp = open(filename, 'r')\n",
    "    data=json.load(fp)\n",
    "    return len([el for el in data['Items'] if el['data']['language'] == 'en'])\n",
    "\n",
    "print('Q1 (b):')\n",
    "num1 = test_q1b_specific(0)\n",
    "num2 = test_q1b_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 (c):\n",
      "specific case: 5649\n",
      "generic case: 5649\n"
     ]
    }
   ],
   "source": [
    "def test_q1c_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results: \n",
    "    In the JSON file for September 2008, how many English-language articles in this month mention any of the five companies: C.N, JPM.N, BAC.N, GS.N and MS.N?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    num = 5649\n",
    "    return num\n",
    "\n",
    "def test_q1c_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    In the JSON file named \"filename\", how many English-language articles in this month mention any of the five companies: C.N, JPM.N, BAC.N, GS.N and MS.N?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    fc = set(['R:C.N','R:JPM.N','R:BAC.N','R:GS.N','R:MS.N'])\n",
    " \n",
    "    return len([el for el in data['Items'] if ((el['data']['language'] == 'en') and (len(set(el['data']['subjects']) & fc) > 0))])\n",
    "\n",
    "\n",
    "print('Q1 (c):')\n",
    "num1 = test_q1c_specific(0)\n",
    "num2 = test_q1c_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 (d):\n",
      "specific case: [array(['2008-09-01T00:37:25.000Z', '2008-09-01T01:17:04.000Z',\n",
      "       '2008-09-01T09:59:15.000Z'], dtype='<U24'), array(['2008-09-30T20:50:37.000Z', '2008-09-30T15:58:12.000Z',\n",
      "       '2008-09-30T13:29:56.000Z'], dtype='<U24'), array(['PRESS DIGEST - South Korean newspapers - Sept 1',\n",
      "       'UPDATE 1-Lehman in talks with KDB to raise $6 bln-Telegraph',\n",
      "       'TAKE-A-LOOK-Ongoing major Asia M&A deals'], dtype='<U59'), array(['S&P: 3 LEHMAN BROS F-R COMM MTG TR 2007-LLF C5 RTGS ON WTCHNEG',\n",
      "       'FUND VIEW-Aviva Investors buys junk debt as spreads soar',\n",
      "       \"S&P CUTS, WITHDRAWS RTGS ON LEHMAN BROS. TREASURY CO.'S NTS\"],\n",
      "      dtype='<U62')]\n",
      "generic case: [array(['2008-09-01T00:37:25.000Z', '2008-09-01T01:17:04.000Z',\n",
      "       '2008-09-01T09:59:15.000Z'], dtype='<U24'), array(['2008-09-30T20:50:37.000Z', '2008-09-30T15:58:12.000Z',\n",
      "       '2008-09-30T13:29:56.000Z'], dtype='<U24'), array(['PRESS DIGEST - South Korean newspapers - Sept 1',\n",
      "       'UPDATE 1-Lehman in talks with KDB to raise $6 bln-Telegraph',\n",
      "       'TAKE-A-LOOK-Ongoing major Asia M&A deals'], dtype='<U59'), array(['S&P: 3 LEHMAN BROS F-R COMM MTG TR 2007-LLF C5 RTGS ON WTCHNEG',\n",
      "       'FUND VIEW-Aviva Investors buys junk debt as spreads soar',\n",
      "       \"S&P CUTS, WITHDRAWS RTGS ON LEHMAN BROS. TREASURY CO.'S NTS\"],\n",
      "      dtype='<U62')]\n"
     ]
    }
   ],
   "source": [
    "def test_q1d_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results: \n",
    "    In the JSON file for September 2008:\n",
    "    Find all English-language articles in the month with at least 1,800 characters in their body, and that are tagged as being about Lehman Brothers (ticker LEH.N). (You'll find that many of these articles have duplicated altId's.) Now for each altId that appears in these articles (i.e., for each unique article chain), find the first article in the chain. Sort all these first-in-chain articles by their versionCreated date, and show the versionCreated dates and headlines of the first and last three of these articles in the month.\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    # the first and last three versionCreated dates\n",
    "    # return an np.array of strings\n",
    "    version_first3 = np.array(['2008-09-01T00:37:25.000Z', '2008-09-01T01:17:04.000Z', '2008-09-01T09:59:15.000Z'])\n",
    "    version_last3 = np.array(['2008-09-30T20:50:37.000Z', '2008-09-30T15:58:12.000Z', '2008-09-30T13:29:56.000Z'])\n",
    "    # the first and last three headlines\n",
    "    # return an np.array of strings\n",
    "    headline_first3 = np.array(['PRESS DIGEST - South Korean newspapers - Sept 1', 'UPDATE 1-Lehman in talks with KDB to raise $6 bln-Telegraph', 'TAKE-A-LOOK-Ongoing major Asia M&A deals'])\n",
    "    headline_last3 = np.array(['S&P: 3 LEHMAN BROS F-R COMM MTG TR 2007-LLF C5 RTGS ON WTCHNEG', 'FUND VIEW-Aviva Investors buys junk debt as spreads soar', \"S&P CUTS, WITHDRAWS RTGS ON LEHMAN BROS. TREASURY CO.'S NTS\"])\n",
    "    \n",
    "\n",
    "    return [version_first3, version_last3, headline_first3, headline_last3]\n",
    "\n",
    "def test_q1d_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    In the JSON file named \"filename\":\n",
    "    Find all English-language articles in the month with at least 1,800 characters in their body, and that are tagged as being about Lehman Brothers (ticker LEH.N). (You'll find that many of these articles have duplicated altId's.) Now for each altId that appears in these articles (i.e., for each unique article chain), find the first article in the chain. Sort all these first-in-chain articles by their versionCreated date, and show the versionCreated dates and headlines of the first and last three of these articles in the month.\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "\n",
    "    articleid = [el['data']['altId'] for el in data['Items'] if el['data']['language']=='en' and \\\n",
    "            'R:LEH.N' in el['data']['subjects'] and len(el['data']['body']) > 1800]\n",
    "    \n",
    "    f_art = []\n",
    "    for altid in set(articleid):\n",
    "        chain = [(altid, el['data']['versionCreated'],el['data']['headline']) for el in data['Items'] if el['data']['altId']==altid]\n",
    "        sorted_chain = sorted(chain,key=lambda xx: xx[1])\n",
    "        f_art.append(sorted_chain[0])\n",
    "    \n",
    "    f_art = sorted(f_art,key=lambda xx: xx[1])\n",
    "    f_art = np.array(f_art)\n",
    "    # the first and last three versionCreated dates\n",
    "    # return an np.array of strings\n",
    "    version_first3 = np.array([f_art[:3][i][1] for i in range(3)])\n",
    "    version_last3 =  np.array([f_art[-3:][-i][1] for i in range(1,4)])\n",
    "    # the first and last three headlines\n",
    "    # return an np.array of strings\n",
    "    headline_first3 = np.array([f_art[:3][i][2] for i in range(3)])\n",
    "    headline_last3 = np.array([f_art[-3:][-i][2] for i in range(1,4)])\n",
    "    \n",
    "    return [version_first3, version_last3, headline_first3, headline_last3]\n",
    "\n",
    "\n",
    "print('Q1 (d):')\n",
    "num1 = test_q1d_specific(0)\n",
    "num2 = test_q1d_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 (a):\n",
      "specific case: 6915\n",
      "generic case: 6915\n"
     ]
    }
   ],
   "source": [
    "def test_q2_getdata(self, data):\n",
    "    '''\n",
    "    (optional)\n",
    "    Implement a generic function that loads the EM articles in the loaded JSON file.\n",
    "    specifically, the function \n",
    "        - takes the loaded json file (by json.load) as the input and \n",
    "        - returns the indices for the EM articles as an np.array\n",
    "    You may need this function to make the rest of the codes for Q2 more compact (see test_q2a_generic for more details)\n",
    "    '''\n",
    "    # TODO: implement me\n",
    "    countrys = set(['N2:BR','N2:MX','N2:TR','N2:ZA'])\n",
    "    \n",
    "    return np.array([i for i,el in enumerate(data['Items']) if el['data']['language']=='en' and (len(countrys & set(el['data']['subjects'])) > 0)])\n",
    "\n",
    "def test_q2a_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results: \n",
    "    How many EM articles are there in the JSON file for September 2008?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    return 6915\n",
    "\n",
    "def test_q2a_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    How many EM articles are there in the JSON file named \"filename\"?\n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    # This function is already complete, and you only need to implement the \"test_q2_getdata\" method\n",
    "    # An alternative is to implement your own codes if you prefer not to work with \"test_q2_getdata\"\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q2_getdata(0,data)\n",
    "    return len(idx)\n",
    "\n",
    "print('Q2 (a):')\n",
    "num1 = test_q2a_specific(0)\n",
    "num2 = test_q2a_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 (b):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_q2_getdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ2 (b):\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m num1 \u001b[38;5;241m=\u001b[39m test_q2b_specific(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m num2 \u001b[38;5;241m=\u001b[39m test_q2b_generic(\u001b[38;5;241m0\u001b[39m,file_q1q2)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecific case:\u001b[39m\u001b[38;5;124m'\u001b[39m, num1)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneric case:\u001b[39m\u001b[38;5;124m'\u001b[39m, num2)\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mtest_q2b_generic\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     44\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fp)\n\u001b[0;32m---> 46\u001b[0m idx \u001b[38;5;241m=\u001b[39m test_q2_getdata(\u001b[38;5;241m0\u001b[39m,data)\n\u001b[1;32m     47\u001b[0m els \u001b[38;5;241m=\u001b[39m [data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItems\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]\n\u001b[1;32m     48\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(tokenizer\u001b[38;5;241m=\u001b[39m analyzer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_q2_getdata' is not defined"
     ]
    }
   ],
   "source": [
    "def analyzer(txt, stem=True):\n",
    "    '''\n",
    "    (optional)\n",
    "    you may want to implement an analyzer that conducts prelimanry cleaning of the data\n",
    "    this method is proposed to help you streamline the codes and is optional\n",
    "    '''\n",
    "    # TODO: implement the following:\n",
    "    # lowercase\n",
    "    # replace special characters and punctuations\n",
    "    # replace white spaces \n",
    "    # replace numbers \n",
    "    # remove stop words\n",
    "    txt = re.sub('[0-9][.,0-9]*','_n_',txt)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    analyzer = CountVectorizer(stop_words='english').build_analyzer()\n",
    "    if stem:\n",
    "        # this makes your analyzer more generic\n",
    "        # in the sentiment analysis, you don't want to stem the words\n",
    "        return [stemmer.stem(wrd) for wrd in analyzer(txt)]\n",
    "    return [wrd for wrd in analyzer(txt)]\n",
    "\n",
    "def test_q2b_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results:\n",
    "    In the JSON file for September 2008:\n",
    "    What are the 25 most frequently occurring tokens (words or symbols that may not have been captured by the data cleaning step) in each month? \n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    # return an np.array of strings, each corresponding to the words\n",
    "    topwords = np.array(['_n_', 'reuter','said', 'com', 'percent', 'market', 'price',\n",
    "                            'year', 'gold', 'sept','report', 'bank' ,'news', 'pct' ,\n",
    "                            'keyword', 'net' ,'south', 'messag', 'africa', 'new',\n",
    "                            'rate', 'oil', 'mexico' ,'trade','week'])\n",
    "    return topwords\n",
    "\n",
    "\n",
    "def test_q2b_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    In the JSON file named \"filename\":\n",
    "    What are the 25 most frequently occurring tokens (words or symbols that may not have been captured by the data cleaning step) in each month? \n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q2_getdata(0,data)\n",
    "    els = [data['Items'][i] for i in idx]\n",
    "    cv = CountVectorizer(tokenizer= analyzer)\n",
    "    dtm_raw = cv.fit_transform([el['data']['body'] for el in els if el['data']['body'] != ''])\n",
    "\n",
    "    freq = dtm_raw.sum(axis=0)\n",
    "    nfreq = [(wrd,freq[0,idx]) for wrd,idx in cv.vocabulary_.items()]\n",
    "    nfreq = sorted(nfreq,key = lambda xx: xx[1],reverse=True)\n",
    "\n",
    "    # return an np.array of strings, each corresponding to the words\n",
    "    topwords = np.array([nfreq[i][0] for i in range(25)])\n",
    "    return topwords\n",
    "\n",
    "print('Q2 (b):')\n",
    "num1 = test_q2b_specific(0)\n",
    "num2 = test_q2b_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 (c):\n",
      "specific case: [0.30562239 0.05262709 0.03006504 0.02317345 0.02301426 0.0175116\n",
      " 0.01525055 0.01380163 0.01278647 0.0124427  0.01214968 0.01090841\n",
      " 0.01050696 0.01021856 0.00998553 0.00974789 0.00921724 0.00892884\n",
      " 0.00891038 0.00882502 0.00875119 0.00771756 0.00769218 0.00744993\n",
      " 0.00727689 0.00724459 0.00714769 0.00700464 0.0068962  0.00685929\n",
      " 0.00676008 0.00671163 0.00644399 0.00627787 0.00604485 0.00579336\n",
      " 0.00577952 0.00572876 0.00569646 0.00569415 0.00567339 0.00554188\n",
      " 0.00540345 0.00540114 0.00521887 0.00519119 0.00518657 0.00517965\n",
      " 0.00516119 0.00508505 0.00501353 0.00501353 0.00500892 0.00497431\n",
      " 0.00484511 0.00483588 0.00483588 0.00481281 0.0047459  0.00474128\n",
      " 0.00467207 0.00460747 0.00454517 0.00444596 0.00441828 0.0043606\n",
      " 0.00435829 0.0042983  0.00429599 0.00426831 0.00420601 0.0042014\n",
      " 0.00415064 0.00412988 0.00394069 0.00387147 0.00387147 0.00386455\n",
      " 0.00384378 0.00384148 0.00384148 0.00383686 0.0038161  0.00380687\n",
      " 0.00377688 0.00377226 0.00374227 0.00373073 0.00372612 0.00370766\n",
      " 0.00367536 0.0036569  0.00364998 0.00360845 0.00358538 0.00354385\n",
      " 0.00351155 0.00344695 0.00342618 0.00341234]\n",
      "generic case: [0.3056224  0.05262709 0.03006504 0.02317345 0.02301426 0.0175116\n",
      " 0.01525055 0.01380163 0.01278647 0.0124427  0.01214968 0.01090841\n",
      " 0.01050696 0.01021856 0.00998553 0.00974789 0.00921724 0.00892884\n",
      " 0.00891038 0.00882502 0.00875119 0.00771756 0.00769218 0.00744993\n",
      " 0.00727689 0.00724459 0.00714769 0.00700464 0.0068962  0.00685929\n",
      " 0.00676008 0.00671163 0.00644399 0.00627787 0.00604485 0.00579336\n",
      " 0.00577952 0.00572876 0.00569646 0.00569415 0.00567339 0.00554188\n",
      " 0.00540345 0.00540114 0.00521887 0.00519119 0.00518657 0.00517965\n",
      " 0.00516119 0.00508505 0.00501353 0.00501353 0.00500892 0.00497431\n",
      " 0.00484511 0.00483588 0.00483588 0.00481281 0.0047459  0.00474128\n",
      " 0.00467207 0.00460747 0.00454517 0.00444596 0.00441828 0.0043606\n",
      " 0.00435829 0.0042983  0.00429599 0.00426831 0.00420601 0.0042014\n",
      " 0.00415064 0.00412988 0.00394069 0.00387147 0.00387147 0.00386455\n",
      " 0.00384378 0.00384148 0.00384148 0.00383686 0.0038161  0.00380687\n",
      " 0.00377688 0.00377226 0.00374227 0.00373073 0.00372612 0.00370766\n",
      " 0.00367536 0.0036569  0.00364998 0.00360845 0.00358538 0.00354385\n",
      " 0.00351155 0.00344695 0.00342618 0.00341234]\n"
     ]
    }
   ],
   "source": [
    "def test_q2c_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results:\n",
    "    In the JSON file for September 2008:\n",
    "    For the 100 most frequently appearing tokens, sort the tokens by frequency of occurrence from highest to lowest. What fraction of all token occurrences are accounted for by the top 100 tokens?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    # hint: you might want to hard-code the frequencies instead of the fractions for better precision\n",
    "    top_freq = np.array([0.3056224,  0.05262709, 0.03006504, 0.02317345, 0.02301426, 0.0175116,\\\n",
    " 0.01525055, 0.01380163, 0.01278647, 0.0124427 , 0.01214968, 0.01090841,\n",
    " 0.01050696, 0.01021856, 0.00998553, 0.00974789, 0.00921724, 0.00892884,\n",
    " 0.00891038, 0.00882502, 0.00875119, 0.00771756, 0.00769218, 0.00744993,\n",
    " 0.00727689, 0.00724459, 0.00714769, 0.00700464, 0.0068962 , 0.00685929,\n",
    " 0.00676008, 0.00671163, 0.00644399, 0.00627787, 0.00604485, 0.00579336,\n",
    " 0.00577952, 0.00572876, 0.00569646, 0.00569415, 0.00567339, 0.00554188,\n",
    " 0.00540345, 0.00540114, 0.00521887, 0.00519119, 0.00518657, 0.00517965,\n",
    " 0.00516119, 0.00508505, 0.00501353, 0.00501353, 0.00500892, 0.00497431,\n",
    " 0.00484511, 0.00483588, 0.00483588, 0.00481281, 0.0047459 , 0.00474128,\n",
    " 0.00467207, 0.00460747, 0.00454517, 0.00444596, 0.00441828, 0.0043606,\n",
    " 0.00435829, 0.0042983 , 0.00429599, 0.00426831, 0.00420601, 0.0042014,\n",
    " 0.00415064, 0.00412988, 0.00394069, 0.00387147, 0.00387147, 0.00386455,\n",
    " 0.00384378, 0.00384148, 0.00384148, 0.00383686, 0.0038161 , 0.00380687,\n",
    " 0.00377688, 0.00377226, 0.00374227, 0.00373073, 0.00372612, 0.00370766,\n",
    " 0.00367536, 0.0036569 , 0.00364998, 0.00360845, 0.00358538, 0.00354385,\n",
    " 0.00351155, 0.00344695, 0.00342618, 0.00341234])\n",
    "    return top_freq/sum(top_freq)\n",
    "\n",
    "\n",
    "def test_q2c_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    In the JSON file named \"filename\":\n",
    "    For the 100 most frequently appearing tokens, sort the tokens by frequency of occurrence from highest to lowest. What fraction of all token occurrences are accounted for by the top 100 tokens?\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q2_getdata(0,data)\n",
    "    els = [data['Items'][i] for i in idx]\n",
    "    cv = CountVectorizer(tokenizer= analyzer)\n",
    "    dtm_raw = cv.fit_transform([el['data']['body'] for el in els if el['data']['body'] != ''])\n",
    "\n",
    "    freq = dtm_raw.sum(axis=0)\n",
    "    nfreq = [(wrd,freq[0,idx]) for wrd,idx in cv.vocabulary_.items()]\n",
    "    nfreq = sorted(nfreq,key = lambda xx: xx[1],reverse=True)\n",
    "\n",
    "    # return an np.array of strings, each corresponding to the words\n",
    "    top_freq = np.array([nfreq[i][1] for i in range(100)])\n",
    "    # return topwords\n",
    "    return top_freq/sum(top_freq)\n",
    "\n",
    "print('Q2 (c):')\n",
    "num1 = test_q2c_specific(0)\n",
    "num2 = test_q2c_generic(0,file_q1q2)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 (d):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_q2d_open(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: question 2 part d\n",
    "    \"\"\"\n",
    "    # TODO: implement me\n",
    "    # apart from finishing the codes here, please also upload the graph named \"q2d.png\" as a part of the submission. \n",
    "    fp = open(filename, 'r')\n",
    "    fig, ax = plt.subplots()\n",
    "    data = json.load(fp)\n",
    "    idx = test_q2_getdata(0,data)\n",
    "    els = [data['Items'][i] for i in idx]\n",
    "    cv = CountVectorizer(tokenizer= analyzer)\n",
    "    dtm_raw = cv.fit_transform([el['data']['body'] for el in els if el['data']['body'] != ''])\n",
    "\n",
    "    freq = dtm_raw.sum(axis=0)\n",
    "    nfreq = [(wrd,freq[0,idx]) for wrd,idx in cv.vocabulary_.items()]\n",
    "    nfreq = sorted(nfreq,key = lambda xx: xx[1],reverse=True)\n",
    "    freq = np.array([el[1] for el in nfreq])\n",
    "    freq = freq/sum(freq)\n",
    "    plt.plot(freq[:1001])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Q2 (d):')\n",
    "test_q2d_open(0,file_q1q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 (e):\n",
      "specific case: [0.16657939 1.06916973]\n",
      "generic case: [0.16657939 1.06916973]\n"
     ]
    }
   ],
   "source": [
    "def test_q2e_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results:\n",
    "    In the JSON file for September 2008: answer Question 2 part e\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "\n",
    "    return np.array([0.16657939, 1.06916973])\n",
    "\n",
    "\n",
    "def test_q2e_generic(self, filename: str, lm_dict_file = 'Loughran-McDonald_MasterDictionary_1993-2021.csv'):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    In the JSON file named \"filename\": answer Question 2 part e\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    lm_dict = pd.read_csv(lm_dict_file)\n",
    "    lm_dict = lm_dict.dropna()\n",
    "    lm_dict['Word'] = lm_dict.apply(lambda df: df['Word'].lower(), axis=1)\n",
    "    pos = set(lm_dict[lm_dict['Positive']>0]['Word'])\n",
    "    neg = set(lm_dict[lm_dict['Negative']>0]['Word'])\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q2_getdata(0,data)\n",
    "    els = [data['Items'][i] for i in idx]\n",
    "\n",
    "    score_list = []\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "\n",
    "    for el in els:\n",
    "\n",
    "        anz = analyzer(el['data']['body'],stem = False)\n",
    "        if len(anz) == 0:\n",
    "            score_list.append(0)\n",
    "            pos_list.append(0)\n",
    "            neg_list.append(0)\n",
    "\n",
    "            continue\n",
    "        pos_num = sum(token in pos for token in anz )\n",
    "        neg_num = sum(token in neg for token in anz ) \n",
    "        # print(pos_num,neg_num)\n",
    "        score = (pos_num - neg_num)/len(anz)\n",
    "        pos_pct = pos_num/len(anz) \n",
    "        neg_pct = neg_num/len(anz)\n",
    "\n",
    "        score_list.append(score)\n",
    "        pos_list.append(pos_pct)\n",
    "        neg_list.append(neg_pct)\n",
    "\n",
    "    vpos = np.var(pos_list)\n",
    "    vneg = np.var(neg_list)\n",
    "    cov = float(np.cov(pos_list,neg_list)[0,1])\n",
    "    vscore = vpos + vneg - 2 * cov\n",
    "    \n",
    "    pos_var_percentage = vpos/vscore\n",
    "    neg_var_percentage = vneg/vscore\n",
    "\n",
    "    return np.array([pos_var_percentage, neg_var_percentage])\n",
    "\n",
    "\n",
    "\n",
    "print('Q2 (e):')\n",
    "num1 = test_q2e_specific(0)\n",
    "num2 = test_q2e_generic(0,file_q1q2, lm_dict_file)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 (f):\n",
      "9 6\n",
      "specific case: (array([\"TEXT-Moody's release on Banco Fibra SA\",\n",
      "       'WRAPUP 1-Soccer-UEFA Cup relief for Milan and Ancelotti',\n",
      "       'TEXT-Fitch release on Finansbank A.S.',\n",
      "       \"TEXT-Fitch may raise Brazil's Company SA\",\n",
      "       'TEXT-S&P report on Turkish transparency and disclosure survey'],\n",
      "      dtype='<U61'), array([\"FACTBOX-S.Africa's Zuma seeks dismissal of graft charges\",\n",
      "       'INSTANT VIEW-Reaction to dismissal of case against Zuma',\n",
      "       'Media-government tensions flare in Turkey',\n",
      "       'Media-government tensions flare in Turkey',\n",
      "       'Mexico peso slammed by bailout fears, stocks down'], dtype='<U56'))\n",
      "generic case: (array([\"TEXT-Moody's release on Banco Fibra SA\",\n",
      "       'WRAPUP 1-Soccer-UEFA Cup relief for Milan and Ancelotti',\n",
      "       'TEXT-Fitch release on Finansbank A.S.',\n",
      "       \"TEXT-Fitch may raise Brazil's Company SA\",\n",
      "       'TEXT-S&P report on Turkish transparency and disclosure survey'],\n",
      "      dtype='<U61'), array([\"FACTBOX-S.Africa's Zuma seeks dismissal of graft charges\",\n",
      "       'INSTANT VIEW-Reaction to dismissal of case against Zuma',\n",
      "       'Media-government tensions flare in Turkey',\n",
      "       'Media-government tensions flare in Turkey',\n",
      "       'Mexico peso slammed by bailout fears, stocks down'], dtype='<U56'))\n"
     ]
    }
   ],
   "source": [
    "def test_q2f_specific(self):\n",
    "    \"\"\"\n",
    "    now answer this question by hard-coding your results:\n",
    "    In the JSON file for September 2008: \n",
    "    For articles between 300 and 500 words in length (this is the length after stop words have been dropped), find the headlines of the 5 most negative and most positive sentiment articles. (It is okay if some of these articles have duplicated altId's.)\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, return by hard-coding the answer\n",
    "    # return an np.array of strings\n",
    "    top5_hl = np.array([\"TEXT-Moody's release on Banco Fibra SA\",\n",
    "    'WRAPUP 1-Soccer-UEFA Cup relief for Milan and Ancelotti',\n",
    "    'TEXT-Fitch release on Finansbank A.S.',\n",
    "    \"TEXT-Fitch may raise Brazil's Company SA\",\n",
    "    'TEXT-S&P report on Turkish transparency and disclosure survey'])\n",
    "    bot5_hl = np.array([\"FACTBOX-S.Africa's Zuma seeks dismissal of graft charges\",\n",
    "    'INSTANT VIEW-Reaction to dismissal of case against Zuma',\n",
    "    'Media-government tensions flare in Turkey',\n",
    "    'Media-government tensions flare in Turkey',\n",
    "    'Mexico peso slammed by bailout fears, stocks down'])\n",
    "    return top5_hl, bot5_hl\n",
    "\n",
    "\n",
    "def test_q2f_generic(self, filename: str, lm_dict_file: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    In the JSON file named \"filename\":\n",
    "    For articles between 300 and 500 words in length (this is the length after stop words have been dropped), find the headlines of the 5 most negative and most positive sentiment articles. (It is okay if some of these articles have duplicated altId's.)\n",
    "    \"\"\"\n",
    "    # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "    lm_dict = pd.read_csv(lm_dict_file)\n",
    "    lm_dict = lm_dict.dropna()\n",
    "    lm_dict['Word'] = lm_dict.apply(lambda df: df['Word'].lower(), axis=1)\n",
    "    pos = set(lm_dict[lm_dict['Positive']>0]['Word'])\n",
    "    neg = set(lm_dict[lm_dict['Negative']>0]['Word'])\n",
    "\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q2_getdata(0,data)\n",
    "    els = [data['Items'][i] for i in idx]\n",
    "\n",
    "    els3_5 = [el for el in els if len(analyzer(el['data']['body'],stem = False)) >= 300 and len(analyzer(el['data']['body'],stem = False)) <= 500 ]\n",
    "\n",
    "    sentilist = []\n",
    "    for el in els3_5:\n",
    "        anz = analyzer(el['data']['body'],stem = False)\n",
    "        pos_num = sum(token in pos for token in anz )\n",
    "        neg_num = sum(token in neg for token in anz ) \n",
    "        score = (pos_num - neg_num)/len(anz)\n",
    "        sentilist.append((el['data']['headline'],score))\n",
    "    print(pos_num,neg_num)\n",
    "    sorted_sent = sorted(sentilist,key=lambda xx: xx[1],reverse = True)\n",
    "    \n",
    "    # return an np.array of strings\n",
    "    top5_hl = np.array([sorted_sent[i][0] for i in range(5)])\n",
    "    bot5_hl = np.array([sorted_sent[-i][0] for i in range(1,6)])\n",
    "\n",
    "    return top5_hl, bot5_hl\n",
    "\n",
    "\n",
    "print('Q2 (f):')\n",
    "num1 = test_q2f_specific(0)\n",
    "num2 = test_q2f_generic(0,file_q1q2, lm_dict_file)\n",
    "print('specific case:', num1)\n",
    "print('generic case:', num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 (a):\n",
      "59322\n"
     ]
    }
   ],
   "source": [
    "def test_q3_getdata(self, data):\n",
    "    '''\n",
    "    (optional)\n",
    "    Implement a generic function that loads the English language articles that mention the words \"COVID\" or \"coronavirus\" in their body\n",
    "    specifically, the function \n",
    "        - takes the loaded json file (by json.load) as the input and \n",
    "        - returns the indices for the covid articles.\n",
    "    You may need this function to make the rest of the codes for Q3 more compact\n",
    "    '''\n",
    "    # TODO: implement me\n",
    "    data2 = [i for i,el in enumerate(data['Items']) if el['data']['language'] == 'en' and ('coronavirus' in el['data']['body'].lower()) or ('covid' in el['data']['body'].lower())]\n",
    "    return data2\n",
    "\n",
    "\n",
    "def test_q3a_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    How many articles did you locate that satisfy the search criteria in the JSON file named \"filename\"?\n",
    "    \"\"\"\n",
    "    # TODO: This function is already complete, and you need to implement the \"test_q3_getdata\" method\n",
    "    # An alternative is to implement your own codes if you prefer not to work with \"test_q2_getdata\"\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q3_getdata(0,data)\n",
    "    return len(idx)\n",
    "\n",
    "print('Q3 (a):')\n",
    "num = test_q3a_generic(0,file_q3)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 (b):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wolong/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([['_n_', '1158550'],\n",
      "       ['news', '382350'],\n",
      "       ['rate', '193719'],\n",
      "       ...,\n",
      "       ['dijo', '2871'],\n",
      "       ['sentiment', '2868'],\n",
      "       ['spain', '2860']], dtype='<U21'), array([['pratfal', '1'],\n",
      "       ['edberg', '1'],\n",
      "       ['sasgroup', '1'],\n",
      "       ...,\n",
      "       ['einst', '1'],\n",
      "       ['ns_n_n_n_d_n_f', '1'],\n",
      "       ['joust', '1']], dtype='<U47')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_q3b_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    For the JSON file named \"filename\", \n",
    "    Find the 1000 most frequently occurring words. Show the top and bottom most frequent words from this list of 100\n",
    "    \"\"\"\n",
    "    # TODO: implement me\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = test_q3_getdata(0,data)\n",
    "    covidel = [data['Items'][i] for i in idx]\n",
    "    cv = CountVectorizer(tokenizer= analyzer)    \n",
    "    dtm_raw_covid = cv.fit_transform([el['data']['body'] for el in covidel if el['data']['body'] != ''])\n",
    "    \n",
    "    freq = dtm_raw_covid.sum(axis=0)\n",
    "    nfreq = [(wrd,freq[0,idx]) for wrd,idx in cv.vocabulary_.items()]  \n",
    "    nfreq = sorted(nfreq, key = lambda xx: xx[1], reverse=True)\n",
    "\n",
    "    topwords = np.array(nfreq[:1000])\n",
    "    botwords = np.array(nfreq[-1000:])\n",
    "    \n",
    "    return [topwords, botwords]\n",
    "\n",
    "\n",
    "print('Q3 (b):')\n",
    "tops = test_q3b_generic(0,file_q3)\n",
    "print(tops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 (c):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ3 (c):\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m topwords, lda10, feature_names \u001b[38;5;241m=\u001b[39m calc\u001b[38;5;241m.\u001b[39mtest_q3c_generic(file_q3)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(topwords)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ3 (d):\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calc' is not defined"
     ]
    }
   ],
   "source": [
    "def test_q3c_generic(self, filename: str):\n",
    "    \"\"\"\n",
    "    now answer this question: \n",
    "    For the JSON file named \"filename\", \n",
    "    Show the output of the model estimation as a list of the 20 words in each topic that have the highest topic-word probability\n",
    "    return as a dictionary of the following format\n",
    "    \"\"\"\n",
    "    # TODO: implement me\n",
    "    fp = open(filename, 'r')\n",
    "    data = json.load(fp)\n",
    "    idx = self.test_q3_getdata(data)\n",
    "    covidel = [data['Items'][i] for i in idx]\n",
    "    cv = CountVectorizer(tokenizer= self.analyzer)  \n",
    "    \n",
    "    dtm_raw = cv.fit_transform([el['data']['body'] for el in covidel])\n",
    "    Nwrds = 1000\n",
    "    freq = dtm_raw.sum(axis=0)\n",
    "    top_words = [(wrd,idx,freq[0,idx]) for wrd,idx in cv.vocabulary_.items()]  \n",
    "    top_words = sorted(top_words, key = lambda xx: xx[2], reverse=True)[:Nwrds]\n",
    "    words_ordered = [wrd for wrd,_,_ in top_words]\n",
    "    \n",
    "    \n",
    "    lda = LDA(n_components=10,learning_method='batch',max_iter=50)\n",
    "    lda.fit(dtm_raw)\n",
    "    \n",
    "    topwords = dict()\n",
    "    \n",
    "    for ii in range(len(lda.components_)):\n",
    "        tot_refs = sum(lda.components_[ii]) ## this comes back as a word count\n",
    "        ## need to normalize to get the probabilities\n",
    "        topic = [(wrd,refs/tot_refs) for wrd,refs in zip(words_ordered,lda.components_[ii])]\n",
    "        ## sort by probability\n",
    "        topic = sorted(topic,key = lambda xx: xx[1],reverse=True)\n",
    "        topwords[ii+1] = [wrd for wrd,prob in topic[:20]]\n",
    "\n",
    "    \n",
    "    lda10 = None\n",
    "    feature_names = np.array([])\n",
    "    \n",
    "    # the ideal outputs for the purpose of being used later in q3d are respectivley:\n",
    "    #     topwords: this is what we need to grade this question, so make sure this is correct\n",
    "    #     lda10: the instance of the trained lda model (optional)\n",
    "    #     feature_names: the words associated with the model (optional)\n",
    "    # the last two are designed to help you make your codes more compact\n",
    "    # it's okay if you want to opt out of the last two outputs: just keep them empty as None and an empty array\n",
    "    return [topwords, lda10, feature_names]\n",
    "\n",
    "\n",
    "print('Q3 (c):')\n",
    "topwords, lda10, feature_names = test_q3c_generic(0,file_q3)\n",
    "print(topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CalculatorException(Exception):\n",
    "    \"\"\"A class to throw if you come across incorrect syntax or other issues\"\"\"\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class hm_textAn(object):\n",
    "    \"\"\"\n",
    "    This is the class you need to finish. \n",
    "    \"\"\"\n",
    "    def analyzer(self, txt, stem=True):\n",
    "        '''\n",
    "        (optional)\n",
    "        you may want to implement an analyzer that conducts prelimanry cleaning of the data\n",
    "        this method is proposed to help you streamline the codes and is optional\n",
    "        '''\n",
    "        # TODO: implement the following:\n",
    "        # lowercase\n",
    "        # replace special characters and punctuations\n",
    "        # replace white spaces \n",
    "        # replace numbers \n",
    "        # remove stop words\n",
    "        \n",
    "        if stem:\n",
    "            # this makes your analyzer more generic\n",
    "            # in the sentiment analysis, you don't want to stem the words\n",
    "            pass\n",
    "        return txt\n",
    "\n",
    "    def test_q1a_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results: \n",
    "        How many articles are there in the JSON file for September 2008?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        num = 0\n",
    "        return num\n",
    "    \n",
    "    def test_q1a_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        How many articles are there in the JSON file named \"filename\"?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        fp = open(filename, 'r')\n",
    "        num = 0\n",
    "        return num\n",
    "\n",
    "    def test_q1b_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results: \n",
    "        How many of articles in the JSON file for September 2008 are in English?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        num = 0\n",
    "        return num\n",
    "\n",
    "    def test_q1b_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        How many of articles in the JSON file named \"filename\" are in English?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        fp = open(filename, 'r')\n",
    "        num = 0\n",
    "        return num\n",
    "\n",
    "    def test_q1c_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results: \n",
    "        In the JSON file for September 2008, how many English-language articles in this month mention any of the five companies: C.N, JPM.N, BAC.N, GS.N and MS.N?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        num = 0\n",
    "        return num\n",
    "\n",
    "    def test_q1c_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        In the JSON file named \"filename\", how many English-language articles in this month mention any of the five companies: C.N, JPM.N, BAC.N, GS.N and MS.N?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        fp = open(filename, 'r')\n",
    "        num = 0\n",
    "        return num\n",
    "\n",
    "    def test_q1d_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results: \n",
    "        In the JSON file for September 2008:\n",
    "        Find all English-language articles in the month with at least 1,800 characters in their body, and that are tagged as being about Lehman Brothers (ticker LEH.N). (You'll find that many of these articles have duplicated altId's.) Now for each altId that appears in these articles (i.e., for each unique article chain), find the first article in the chain. Sort all these first-in-chain articles by their versionCreated date, and show the versionCreated dates and headlines of the first and last three of these articles in the month.\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        # the first and last three versionCreated dates\n",
    "        # return an np.array of strings\n",
    "        version_first3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        version_last3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        # the first and last three headlines\n",
    "        # return an np.array of strings\n",
    "        headline_first3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        headline_last3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        \n",
    "        return [version_first3, version_last3, headline_first3, headline_last3]\n",
    "\n",
    "    def test_q1d_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        In the JSON file named \"filename\":\n",
    "        Find all English-language articles in the month with at least 1,800 characters in their body, and that are tagged as being about Lehman Brothers (ticker LEH.N). (You'll find that many of these articles have duplicated altId's.) Now for each altId that appears in these articles (i.e., for each unique article chain), find the first article in the chain. Sort all these first-in-chain articles by their versionCreated date, and show the versionCreated dates and headlines of the first and last three of these articles in the month.\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        fp = open(filename, 'r')\n",
    "        \n",
    "        # the first and last three versionCreated dates\n",
    "        # return an np.array of strings\n",
    "        version_first3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        version_last3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        # the first and last three headlines\n",
    "        # return an np.array of strings\n",
    "        headline_first3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        headline_last3 = np.array(['some_string_1', 'some_string_2', 'some_string_3'])\n",
    "        \n",
    "        return [version_first3, version_last3, headline_first3, headline_last3]\n",
    "\n",
    "    \n",
    "    def test_q2_getdata(self, data):\n",
    "        '''\n",
    "        (optional)\n",
    "        Implement a generic function that loads the EM articles in the loaded JSON file.\n",
    "        specifically, the function \n",
    "            - takes the loaded json file (by json.load) as the input and \n",
    "            - returns the indices for the EM articles as an np.array\n",
    "        You may need this function to make the rest of the codes for Q2 more compact (see test_q2a_generic for more details)\n",
    "        '''\n",
    "        # TODO: implement me\n",
    "        idx = np.array([9999,9999,9999,9999,9999])# probably longer than this...\n",
    "        return idx\n",
    "\n",
    "    def test_q2a_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results: \n",
    "        How many EM articles are there in the JSON file for September 2008?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        num = 0\n",
    "        return num\n",
    "    \n",
    "    def test_q2a_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        How many EM articles are there in the JSON file named \"filename\"?\n",
    "        \"\"\"\n",
    "        # TODO: \n",
    "        # This function is already complete, and you only need to implement the \"test_q2_getdata\" method\n",
    "        # An alternative is to implement your own codes if you prefer not to work with \"test_q2_getdata\"\n",
    "        fp = open(filename, 'r')\n",
    "        data = json.load(fp)\n",
    "        idx = self.test_q2_getdata(data)\n",
    "        return len(idx)\n",
    "\n",
    "\n",
    "    def test_q2b_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results:\n",
    "        In the JSON file for September 2008:\n",
    "        What are the 25 most frequently occurring tokens (words or symbols that may not have been captured by the data cleaning step) in each month? \n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        # return an np.array of strings, each corresponding to the words\n",
    "        topwords = np.array(['some_word' for i in range(25)])\n",
    "        return topwords\n",
    "    \n",
    "    \n",
    "    def test_q2b_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        In the JSON file named \"filename\":\n",
    "        What are the 25 most frequently occurring tokens (words or symbols that may not have been captured by the data cleaning step) in each month? \n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        fp = open(filename, 'r')\n",
    "        \n",
    "        # (hint) use this: cv = CountVectorizer(tokenizer=self.analyzer)\n",
    "        # return an np.array of strings, each corresponding to the words\n",
    "        topwords = np.array(['some_word' for i in range(25)])\n",
    "        return topwords\n",
    "\n",
    "\n",
    "    def test_q2c_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results:\n",
    "        In the JSON file for September 2008:\n",
    "        For the 100 most frequently appearing tokens, sort the tokens by frequency of occurrence from highest to lowest. What fraction of all token occurrences are accounted for by the top 100 tokens?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        # hint: you might want to hard-code the frequencies instead of the fractions for better precision\n",
    "        top_freq = np.array([9999 for i in range(100)])\n",
    "        return top_freq/sum(top_freq)\n",
    "    \n",
    "    \n",
    "    def test_q2c_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        In the JSON file named \"filename\":\n",
    "        For the 100 most frequently appearing tokens, sort the tokens by frequency of occurrence from highest to lowest. What fraction of all token occurrences are accounted for by the top 100 tokens?\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        fp = open(filename, 'r')\n",
    "        top_freq = np.array([9999 for i in range(100)])\n",
    "        return top_freq/sum(top_freq)\n",
    "    \n",
    "    \n",
    "    def test_q2d_open(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: question 2 part d\n",
    "        \"\"\"\n",
    "        # TODO: implement me\n",
    "        # apart from finishing the codes here, please also upload the graph named \"q2d.png\" as a part of the submission. \n",
    "        fp = open(filename, 'r')\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "    def test_q2e_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results:\n",
    "        In the JSON file for September 2008: answer Question 2 part e\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        pos_var_percentage = 0\n",
    "        neg_var_percentage = 0\n",
    "        return np.array([pos_var_percentage, neg_var_percentage])\n",
    "    \n",
    "    \n",
    "    def test_q2e_generic(self, filename: str, lm_dict_file = 'Loughran-McDonald_MasterDictionary_1993-2021.csv'):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        In the JSON file named \"filename\": answer Question 2 part e\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        lm_dict = pd.read_csv(lm_dict_file)\n",
    "        fp = open(filename, 'r')\n",
    "\n",
    "        pos_var_percentage = 0\n",
    "        neg_var_percentage = 0\n",
    "\n",
    "        return np.array([pos_var_percentage, neg_var_percentage])\n",
    "\n",
    "\n",
    "    def test_q2f_specific(self):\n",
    "        \"\"\"\n",
    "        now answer this question by hard-coding your results:\n",
    "        In the JSON file for September 2008: \n",
    "        For articles between 300 and 500 words in length (this is the length after stop words have been dropped), find the headlines of the 5 most negative and most positive sentiment articles. (It is okay if some of these articles have duplicated altId's.)\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, return by hard-coding the answer\n",
    "        # return an np.array of strings\n",
    "        top5_hl = np.array(['some_string_1', 'some_string_2', 'some_string_3', 'some_string_4', 'some_string_5'])\n",
    "        bot5_hl = np.array(['some_string_1', 'some_string_2', 'some_string_3', 'some_string_4', 'some_string_5'])\n",
    "        return top5_hl, bot5_hl\n",
    "    \n",
    "    \n",
    "    def test_q2f_generic(self, filename: str, lm_dict_file: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        In the JSON file named \"filename\":\n",
    "        For articles between 300 and 500 words in length (this is the length after stop words have been dropped), find the headlines of the 5 most negative and most positive sentiment articles. (It is okay if some of these articles have duplicated altId's.)\n",
    "        \"\"\"\n",
    "        # TODO: Implement me, work out the generic codes to load the file and return the answer\n",
    "        lm_dict = pd.read_csv(lm_dict_file)\n",
    "        fp = open(filename, 'r')\n",
    "\n",
    "        # return an np.array of strings\n",
    "        top5_hl = np.array(['some_string_1', 'some_string_2', 'some_string_3', 'some_string_4', 'some_string_5'])\n",
    "        bot5_hl = np.array(['some_string_1', 'some_string_2', 'some_string_3', 'some_string_4', 'some_string_5'])\n",
    "\n",
    "        return top5_hl, bot5_hl\n",
    "    \n",
    "    \n",
    "    def test_q3_getdata(self, data):\n",
    "        '''\n",
    "        (optional)\n",
    "        Implement a generic function that loads the English language articles that mention the words \"COVID\" or \"coronavirus\" in their body\n",
    "        specifically, the function \n",
    "            - takes the loaded json file (by json.load) as the input and \n",
    "            - returns the indices for the covid articles.\n",
    "        You may need this function to make the rest of the codes for Q3 more compact\n",
    "        '''\n",
    "        # TODO: implement me\n",
    "        idx = np.array([9999,9999,9999,9999]) # probably longer than this...\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def test_q3a_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        How many articles did you locate that satisfy the search criteria in the JSON file named \"filename\"?\n",
    "        \"\"\"\n",
    "        # TODO: This function is already complete, and you need to implement the \"test_q3_getdata\" method\n",
    "        # An alternative is to implement your own codes if you prefer not to work with \"test_q2_getdata\"\n",
    "        fp = open(filename, 'r')\n",
    "        data = json.load(fp)\n",
    "        idx = self.test_q3_getdata(data)\n",
    "        return len(idx)\n",
    "    \n",
    "    \n",
    "    def test_q3b_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        For the JSON file named \"filename\", \n",
    "        Find the 1000 most frequently occurring words. Show the top and bottom most frequent words from this list of 100\n",
    "        \"\"\"\n",
    "        # TODO: implement me\n",
    "        fp = open(filename, 'r')\n",
    "        topwords = np.array(['some_word' for i in range(1000)])\n",
    "        botwords = np.array(['some_word' for i in range(1000)])\n",
    "        \n",
    "        return [topwords, botwords]\n",
    "    \n",
    "    \n",
    "    def test_q3c_generic(self, filename: str):\n",
    "        \"\"\"\n",
    "        now answer this question: \n",
    "        For the JSON file named \"filename\", \n",
    "        Show the output of the model estimation as a list of the 20 words in each topic that have the highest topic-word probability\n",
    "        return as a dictionary of the following format\n",
    "        \"\"\"\n",
    "        # TODO: implement me\n",
    "        fp = open(filename, 'r')\n",
    "\n",
    "        topwords = {\n",
    "            1: np.array(['some_word' for i in range(20)]),\n",
    "            2: np.array(['some_word' for i in range(20)]),\n",
    "            3: np.array(['some_word' for i in range(20)]),\n",
    "            4: np.array(['some_word' for i in range(20)]),\n",
    "            5: np.array(['some_word' for i in range(20)]),\n",
    "            6: np.array(['some_word' for i in range(20)]),\n",
    "            7: np.array(['some_word' for i in range(20)]),\n",
    "            8: np.array(['some_word' for i in range(20)]),\n",
    "            9: np.array(['some_word' for i in range(20)]),\n",
    "            10: np.array(['some_word' for i in range(20)]),\n",
    "        }\n",
    "        lda10 = None\n",
    "        feature_names = np.array([])\n",
    "        \n",
    "        # the ideal outputs for the purpose of being used later in q3d are respectivley:\n",
    "        #     topwords: this is what we need to grade this question, so make sure this is correct\n",
    "        #     lda10: the instance of the trained lda model (optional)\n",
    "        #     feature_names: the words associated with the model (optional)\n",
    "        # the last two are designed to help you make your codes more compact\n",
    "        # it's okay if you want to opt out of the last two outputs: just keep them empty as None and an empty array\n",
    "        return [topwords, lda10, feature_names]\n",
    "    \n",
    "\n",
    "    def test_q3d_open(self, filename: str):\n",
    "        '''\n",
    "        Show the output of the two model estimations as word cloud charts. For this, you can use the wordcloud package in Python.\n",
    "        '''\n",
    "        # TODO: implement the codes; you can make use of your codes in q3c, while it's also fine if you opt out of this\n",
    "        # apart from finishing the codes here, please also upload the graph named \"q3d.png\" as a part of the submission. \n",
    "        _, lda10, feature_names = self.test_q3c_generic(filename)\n",
    "\n",
    "\n",
    "    def test_q3e_open(self):\n",
    "        '''\n",
    "        type your response here\n",
    "        '''\n",
    "        response = '''\n",
    "        some response\n",
    "        '''\n",
    "        return response\n",
    "\n",
    "\n",
    "    def test_q3f_open(self):\n",
    "        '''\n",
    "        type your response here\n",
    "        '''\n",
    "        response = '''\n",
    "        some response\n",
    "        '''\n",
    "        return response\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    calc = hm_textAn()\n",
    "    file_q1q2 = 'News.RTRS.200809.0214.txt'\n",
    "    file_q3 = 'News.RTRS.202006.0214.txt'\n",
    "    lm_dict_file = 'Loughran-McDonald_MasterDictionary_1993-2021.csv'\n",
    "\n",
    "    print('Q1 (a):')\n",
    "    num1 = calc.test_q1a_specific()\n",
    "    num2 = calc.test_q1a_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q1 (b):')\n",
    "    num1 = calc.test_q1b_specific()\n",
    "    num2 = calc.test_q1b_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q1 (c):')\n",
    "    num1 = calc.test_q1c_specific()\n",
    "    num2 = calc.test_q1c_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q1 (d):')\n",
    "    num1 = calc.test_q1d_specific()\n",
    "    num2 = calc.test_q1d_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q2 (a):')\n",
    "    num1 = calc.test_q2a_specific()\n",
    "    num2 = calc.test_q2a_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q2 (b):')\n",
    "    num1 = calc.test_q2b_specific()\n",
    "    num2 = calc.test_q2b_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q2 (c):')\n",
    "    num1 = calc.test_q2c_specific()\n",
    "    num2 = calc.test_q2c_generic(file_q1q2)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q2 (d):')\n",
    "    calc.test_q2d_open(file_q1q2)\n",
    "\n",
    "    print('Q2 (e):')\n",
    "    num1 = calc.test_q2e_specific()\n",
    "    num2 = calc.test_q2e_generic(file_q1q2, lm_dict_file)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q2 (f):')\n",
    "    num1 = calc.test_q2f_specific()\n",
    "    num2 = calc.test_q2f_generic(file_q1q2, lm_dict_file)\n",
    "    print('specific case:', num1)\n",
    "    print('generic case:', num2)\n",
    "\n",
    "    print('Q3 (a):')\n",
    "    num = calc.test_q3a_generic(file_q3)\n",
    "    print(num)\n",
    "\n",
    "    print('Q3 (b):')\n",
    "    tops = calc.test_q3b_generic(file_q3)\n",
    "    print(tops)\n",
    "\n",
    "    print('Q3 (c):')\n",
    "    topwords, lda10, feature_names = calc.test_q3c_generic(file_q3)\n",
    "    print(topwords)\n",
    "        \n",
    "    print('Q3 (d):')\n",
    "    calc.test_q3d_open(file_q3)\n",
    "\n",
    "    print('Q3 (e):')\n",
    "    calc.test_q3e_open()\n",
    "\n",
    "    print('Q3 (f):')\n",
    "    calc.test_q3f_open()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
